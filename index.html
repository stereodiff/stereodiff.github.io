<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation">
  <meta name="keywords" content="Video Depth Estimation, Stereo Matching, Video Diffusion Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>StereoDiff</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-1FWSVCGZTG"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-1FWSVCGZTG');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./css/bulma.min.css">
  <link rel="stylesheet" href="./css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./css/bulma-slider.min.css">
  <link rel="stylesheet" href="./css/twentytwenty.css">
  <link rel="stylesheet" href="./css/index.css">
  <link rel="icon" href="./images/icon_circle.jpeg">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <script src="./js/jquery-3.2.1.min.js"></script>
  <script src="./js/jquery.event.move.js"></script>
  <script src="./js/jquery.twentytwenty.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/fontawesome.all.min.js"></script>

  <!--MathJax-->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation</h1>
          <h3 class="title has-text-centered">arXiv 2024</h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://haodong-li.com/" target="_blank" rel="noopener noreferrer">
                Haodong Li<sup>1,2</sup></a>,
            </span>
            <span class="author-block">
                <a href="https://cwchenwang.github.io/" target="_blank" rel="noopener noreferrer">
                Chen Wang<sup>1</sup></a>,
            </span>
            <span class="author-block">
                <a href="https://www.cis.upenn.edu/~leijh/" target="_blank" rel="noopener noreferrer">
                Jiahui Lei<sup>1</sup></a>,
            </span>
            <span class="author-block">
                <a href="https://frank-zy-dou.github.io/" target="_blank" rel="noopener noreferrer">
                Zhiyang Dou<sup>1,3</sup></a>,
            </span>
            <span class="author-block">
                <a href="https://www.cis.upenn.edu/~kostas/" target="_blank" rel="noopener noreferrer">
                Kostas Daniilidis<sup>1</sup></a>,
            </span>
            <span class="author-block">
                <a href="https://jiataogu.me/" target="_blank" rel="noopener noreferrer">
                Jiatao Gu<sup>4</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://lingjie0206.github.io/" target="_blank" rel="noopener noreferrer">
                Lingjie Liu<sup>1&#9993;</sup></a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Pennsylvania</span>
            <span class="author-block"><sup>2</sup>HKUST(GZ)</span>
            <span class="author-block"><sup>3</sup>University of Hong Kong</span>
            <span class="author-block"><sup>4</sup>Apple</span><br>
            <span class="author-block">
                <sup>&#9993;</sup>Corresponding author.
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="./StereoDiff.pdf" target="_blank" rel="noopener noreferrer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <!-- <i class="ai ai-arxiv" style="font-size:23px"></i> -->
                    <i class="fa fa-file-pdf" style="font-size:23px"></i>
                  </span>
                  <span>PDF (39MB)</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1icXyNUaRdpLxeSyB4UkMtDT4JrkVh1O-/view?usp=drive_link" target="_blank" rel="noopener noreferrer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-video" style="font-size:23px"></i>
                  </span>
                  <span>Video Results (158MB)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="" target="_blank" rel="noopener noreferrer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github" style="font-size:23px"></i>
                  </span>
                  <span>
                    Code (Coming Soon)
                  </span>
                </a>
              </span>
              <!-- Hugging Face Space (LCM). -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/spaces/haodongli/Lotus_Depth" target="_blank" rel="noopener noreferrer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon" style="font-size:23px">
                      &#129303;
                  </span>
                  <span>Huggingface Demo</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" width="100%" src="./images/teaser.jpg" alt="Teaser 1"/>
      <h2 class="subtitle">
        <span class="methodname" style="font-weight: bold;">StereoDiff</span> excels in delivering remarkable global and local consistency for video depth estimation. In terms of global consistency, <span class="methodname" style="font-weight: bold;">StereoDiff</span> achieves highly accurate and stable depth maps on static backgrounds across consecutive windows, leveraging stereo matching to prevent the abrupt depth shifts often seen in DepthCrafter, where depth values on static backgrounds can vary significantly between adjacent windows. For local consistency, <span class="methodname" style="font-weight: bold;">StereoDiff</span> yields much smoother, flicker-free depth values across consecutive frames, especially in dynamic regions. In contrast, MonST3R suffers from frequent, pronounced flickering and jitters in these areas.
      </h2>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
        <p>
            Recent video depth estimation methods achieve great performance by following the paradigm of image depth estimation, <i>i.e.</i>, typically fine-tuning pre-trained video diffusion models with massive data.
            However, we argue that video depth estimation is not a naive extension of image depth estimation.
        </p><p>
            The temporal consistency requirements for dynamic and static regions in videos are fundamentally different.
            Consistent video depth in static regions, typically backgrounds, can be more effectively achieved via stereo matching across all frames, which provides much stronger global 3D cues.
            While the consistency for dynamic regions still should be learned from large-scale video depth data to ensure smooth transitions, due to the violation of triangulation constraints.
        </p><p>
            Based on these insights, we introduce <span class="methodname" style="font-weight: bold;">StereoDiff</span>, a two-stage video depth estimator that synergizes stereo matching for mainly the static areas with video depth diffusion for maintaining consistent depth transitions in dynamic areas.
            We mathematically demonstrate how stereo matching and video depth diffusion offer complementary strengths through frequency domain analysis, highlighting the effectiveness of their synergy in capturing the advantages of both.
        </p><p>
            Experimental results on zero-shot, real-world, dynamic video depth benchmarks, both indoor and outdoor, demonstrate <span class="methodname" style="font-weight: bold;">StereoDiff</span>'s SoTA performance, showcasing its superior consistency and accuracy in video depth estimation.
            In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize.
        </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">

          <p>
            Pipeline of <span class="methodname" style="font-weight: bold;">StereoDiff</span>. &#9312; All video frames are paired for stereo matching in the first stage, primarily focusing on static backgrounds, in order to achieve a strong global consistency. &#9313; Using the stereo matching-based video depth from the first stage, the second stage of <span class="methodname" style="font-weight: bold;">StereoDiff</span> applies a video depth diffusion for significantly improving the local consistency without sacrificing its original global consistency, resulting in video depth estimations with both strong global consistency and smooth local consistency.
        </p>
          
          <img id="method_train" width="100%" src="./images/pipe.jpg" alt="Marigold training scheme"/>

        </div>
          <h2 class="title is-3">Experiments</h2>
          <div class="content has-text-justified">

          <p>
            Quantitative comparison of <span class="methodname" style="font-weight: bold;">StereoDiff</span> with SoTA methods on zero-shot, real-world, dynamic video depth benchmarks. The four sections from top to bottom represent: image depth estimators, stereo matching-based estimators, video depth diffusion models, and <span class="methodname" style="font-weight: bold;">StereoDiff</span>.
            To ensure comprehensive evaluation, we used two datasets: Bonn for indoor scenes and KITTI for outdoor scenes.
            We report the mean metric value of <span class="methodname" style="font-weight: bold;">StereoDiff</span> across 10 independent runs.
            Best results are <strong>bolded</strong> and the second best are <u>underlined</u>.
          </p>

          <img id="comparison" width="100%" src="./images/exp_depth.png" alt="Comparison with other methods"/>

          <p class="mt-5">
            Please refer to our paper linked above for more technical details :)
          </p>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre class="selectable"><code>@article{he2024lotus,
    title={Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction},
    author={He, Jing and Li, Haodong and Yin, Wei and Liang, Yixun and Li, Leheng and Zhou, Kaiqiang and Liu, Hongbo and Liu, Bingbing and Chen, Ying-Cong},
    journal={arXiv preprint arXiv:2409.18124},
    year={2024}
}
</code></pre>
  </div>
</section> -->

<footer class="footer pt-4 pb-0">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template based on
            <a href="https://marigoldmonodepth.github.io/">
            Marigold
            </a>
            and licensed under
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
            CC-BY-SA-4.0
            </a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
